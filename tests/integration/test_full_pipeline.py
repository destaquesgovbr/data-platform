"""
Teste de integraÃ§Ã£o completo do pipeline data-platform.

Este teste executa o fluxo completo end-to-end:
1. Limpeza de BDs (PostgreSQL + Typesense)
2. CriaÃ§Ã£o de schema e dados mestre (agencies, themes)
3. Scraping Gov.br (MEC, GESTAO, CGU) - 20 a 23/12/2025
3b. Scraping EBC (Agencia Brasil, TVBrasil) - 19/02/2026
4. Upload para Cogfy (AI enrichment)
5. Enrich com temas do Cogfy
6. GeraÃ§Ã£o de embeddings via API Cloud Run
7. Carga full no Typesense (com embeddings)

Requisitos:
- Docker containers rodando (PostgreSQL + Typesense)
- COGFY_API_KEY configurado
- Cloud Run API de embeddings acessÃ­vel

ExecuÃ§Ã£o:
    poetry run pytest tests/integration/test_full_pipeline.py -v -s
"""

import os
import shutil
import subprocess
import time
from collections.abc import Generator

import psycopg2
import psycopg2.extensions
import pytest
import requests  # type: ignore[import-untyped]

# ==============================================================================
# CONFIGURAÃ‡ÃƒO
# ==============================================================================
DATABASE_URL = os.getenv(
    "DATABASE_URL", "postgresql://destaquesgovbr_dev:dev_password@localhost:5433/destaquesgovbr_dev"
)
START_DATE = "2025-12-20"
END_DATE = "2025-12-23"
AGENCIES = "mec,gestao,cgu"

# EBC usa data diferente porque o scraper e mais lento (visita cada pagina de artigo)
# e datas antigas causam timeout ao paginar ate encontrar artigos no periodo
EBC_DATE = "2026-02-19"
EBC_AGENCIES = "agencia-brasil,tvbrasil"

# Typesense
TYPESENSE_HOST = "localhost"
TYPESENSE_PORT = "8108"
TYPESENSE_API_KEY = "local_dev_key_12345"
TYPESENSE_PROTOCOL = "http"

# Tempo de espera para processamento Cogfy (segundos)
COGFY_WAIT_TIME = 120  # 2 minutos


# ==============================================================================
# FIXTURES
# ==============================================================================
@pytest.fixture(scope="module")  # type: ignore[untyped-decorator]
def docker_services() -> Generator[None, None, None]:
    """
    Inicia containers Docker antes dos testes.

    - Limpa containers existentes
    - Inicia PostgreSQL e Typesense frescos
    - Aguarda serviÃ§os ficarem prontos

    Em CI (GitHub Actions), assume que os containers jÃ¡ estÃ£o rodando como services.
    """
    print("\n" + "=" * 70)
    print("FASE 0: Preparando ambiente Docker")
    print("=" * 70)

    # Detectar se estÃ¡ em CI
    is_ci = os.getenv("CI") == "true" or os.getenv("GITHUB_ACTIONS") == "true"

    if not is_ci:
        # Detectar comando docker compose disponÃ­vel
        docker_compose_cmd = get_docker_compose_command()

        # Limpar containers existentes
        print("ğŸ”§ Parando e removendo containers existentes...")
        subprocess.run(docker_compose_cmd + ["down", "-v"], check=True, cwd=os.getcwd())

        # Iniciar containers limpos
        print("ğŸš€ Iniciando PostgreSQL e Typesense...")
        subprocess.run(docker_compose_cmd + ["up", "-d"], check=True, cwd=os.getcwd())
    else:
        print("â„¹ï¸  Running in CI - using existing service containers...")

    # Aguardar PostgreSQL
    print("â³ Aguardando PostgreSQL ficar pronto...")
    print(f"   DATABASE_URL: {DATABASE_URL}")
    for i in range(30):  # MÃ¡ximo 30 segundos
        try:
            if is_ci:
                # Em CI, testar conexÃ£o direta ao PostgreSQL
                import psycopg2

                conn = psycopg2.connect(DATABASE_URL)
                conn.close()
                print("âœ… PostgreSQL pronto!")
                break
            else:
                # Local, usar docker exec
                result = subprocess.run(
                    [
                        "docker",
                        "exec",
                        "destaquesgovbr-postgres",
                        "pg_isready",
                        "-U",
                        "destaquesgovbr_dev",
                    ],
                    capture_output=True,
                    timeout=5,
                )
                if result.returncode == 0:
                    print("âœ… PostgreSQL pronto!")
                    break
        except (subprocess.TimeoutExpired, Exception) as e:
            if i == 0:  # Print error only once
                print(f"   Tentando conectar... (erro: {e})")
        time.sleep(1)
    else:
        raise RuntimeError("PostgreSQL nÃ£o ficou pronto em 30 segundos")

    # Aguardar Typesense
    print("â³ Aguardando Typesense ficar pronto...")
    for _i in range(30):  # MÃ¡ximo 30 segundos
        try:
            response = requests.get(f"http://{TYPESENSE_HOST}:{TYPESENSE_PORT}/health", timeout=2)
            if response.status_code == 200:
                print("âœ… Typesense pronto!")
                break
        except requests.exceptions.RequestException:
            pass
        time.sleep(1)
    else:
        raise RuntimeError("Typesense nÃ£o ficou pronto em 30 segundos")

    print("\nâœ… Ambiente Docker preparado com sucesso!\n")

    yield

    # Cleanup opcional - manter containers rodando para inspeÃ§Ã£o
    # subprocess.run(["docker-compose", "down"], check=True)


@pytest.fixture(scope="module")  # type: ignore[untyped-decorator]
def env_vars() -> Generator[None, None, None]:
    """Configura variÃ¡veis de ambiente para todos os testes."""
    original_env = os.environ.copy()

    os.environ.update(
        {
            "DATABASE_URL": DATABASE_URL,
            "TYPESENSE_HOST": TYPESENSE_HOST,
            "TYPESENSE_PORT": TYPESENSE_PORT,
            "TYPESENSE_API_KEY": TYPESENSE_API_KEY,
            "TYPESENSE_PROTOCOL": TYPESENSE_PROTOCOL,
            "STORAGE_BACKEND": "postgres",
            "STORAGE_READ_FROM": "postgres",
        }
    )

    yield

    # Restaurar env vars originais
    os.environ.clear()
    os.environ.update(original_env)


# ==============================================================================
# HELPER FUNCTIONS
# ==============================================================================
def run_cli_command(
    command_list: list[str], description: str = ""
) -> subprocess.CompletedProcess[str]:
    """
    Executa comando CLI do data-platform.

    Args:
        command_list: Lista com comando e argumentos
        description: DescriÃ§Ã£o da operaÃ§Ã£o para logging
    """
    if description:
        print(f"\nğŸ“‹ {description}")

    full_command = ["poetry", "run", "data-platform"] + command_list
    print(f"   Executando: {' '.join(full_command)}")

    result = subprocess.run(
        full_command,
        capture_output=True,
        text=True,
        timeout=600,  # 10 minutos timeout
    )

    if result.stdout:
        print(f"   STDOUT: {result.stdout[:500]}")  # Primeiros 500 chars

    if result.returncode != 0:
        print(f"   âŒ ERRO (exit code {result.returncode})")
        if result.stderr:
            print(f"   STDERR: {result.stderr}")
        raise subprocess.CalledProcessError(
            result.returncode, full_command, result.stdout, result.stderr
        )

    print("   âœ… Sucesso")
    return result


def get_db_connection() -> psycopg2.extensions.connection:
    """Retorna conexÃ£o com PostgreSQL."""
    return psycopg2.connect(DATABASE_URL)


def count_news(conn: psycopg2.extensions.connection | None = None, where_clause: str = "") -> int:
    """Conta notÃ­cias no PostgreSQL."""
    close_conn = False
    if conn is None:
        conn = get_db_connection()
        close_conn = True

    cur = conn.cursor()
    query = f"SELECT COUNT(*) FROM news {where_clause}"
    cur.execute(query)
    count = cur.fetchone()[0]
    cur.close()

    if close_conn:
        conn.close()

    return count  # type: ignore[no-any-return]


def get_docker_compose_command() -> list[str]:
    """
    Returns the appropriate docker compose command for the environment.

    Modern Docker uses 'docker compose' (plugin), legacy uses 'docker-compose' (standalone).
    Checks for modern command first, then falls back to legacy.
    """
    # Try modern 'docker compose' first
    if shutil.which("docker"):
        result = subprocess.run(
            ["docker", "compose", "version"],
            capture_output=True,
        )
        if result.returncode == 0:
            return ["docker", "compose"]

    # Fall back to legacy 'docker-compose'
    if shutil.which("docker-compose"):
        return ["docker-compose"]

    raise RuntimeError("Neither 'docker compose' nor 'docker-compose' found")


# ==============================================================================
# TESTES
# ==============================================================================
def test_01_populate_master_data(docker_services: None, env_vars: None) -> None:
    """
    FASE 1: Popula dados mestre (agencies e themes).
    """
    print("\n" + "=" * 70)
    print("FASE 1: Populando dados mestre")
    print("=" * 70)

    # Check if schema already exists (created by docker init.sql)
    print("\nğŸ—„ï¸  Verificando schema do banco de dados...")
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute(
        "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'agencies')"
    )
    schema_exists = cur.fetchone()[0]
    cur.close()
    conn.close()

    if schema_exists:
        print("âœ… Schema jÃ¡ existe (criado pelo Docker init.sql)")
    else:
        print("ğŸ”§ Criando schema do banco de dados...")
        with open("scripts/create_schema.sql") as f:
            schema_sql = f.read()
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute(schema_sql)
        conn.commit()
        cur.close()
        conn.close()
        print("âœ… Schema criado")

    # Popular agencies
    print("\nğŸ“Š Populando agencies...")
    result = subprocess.run(
        [
            "poetry",
            "run",
            "python",
            "scripts/populate_agencies.py",
            "--source",
            "test-data/agencies.yaml",
        ],
        capture_output=True,
        text=True,
        timeout=60,
    )
    assert result.returncode == 0, f"Falha ao popular agencies: {result.stderr}"
    print("âœ… Agencies populadas")

    # Popular themes
    print("\nğŸ“Š Populando themes...")
    result = subprocess.run(
        [
            "poetry",
            "run",
            "python",
            "scripts/populate_themes.py",
            "--source",
            "test-data/themes_tree_enriched_full.yaml",
        ],
        capture_output=True,
        text=True,
        timeout=60,
    )
    assert result.returncode == 0, f"Falha ao popular themes: {result.stderr}"
    print("âœ… Themes populadas")

    # Validar
    conn = get_db_connection()
    cur = conn.cursor()

    cur.execute("SELECT COUNT(*) FROM agencies")
    agencies_count = cur.fetchone()[0]
    assert agencies_count > 0, "Nenhuma agency foi inserida"
    print(f"   ğŸ“ˆ Total de agencies: {agencies_count}")

    cur.execute("SELECT COUNT(*) FROM themes")
    themes_count = cur.fetchone()[0]
    assert themes_count > 0, "Nenhum theme foi inserido"
    print(f"   ğŸ“ˆ Total de themes: {themes_count}")

    cur.close()
    conn.close()

    print("\nâœ… FASE 1 COMPLETA: Dados mestre carregados")


def test_02a_scrape_govbr(docker_services: None, env_vars: None) -> None:
    """
    FASE 2: Scraping de notÃ­cias gov.br (MEC, GESTAO, CGU).
    """
    print("\n" + "=" * 70)
    print(f"FASE 2: Scraping gov.br ({AGENCIES})")
    print("=" * 70)

    run_cli_command(
        [
            "scrape",
            "--start-date",
            START_DATE,
            "--end-date",
            END_DATE,
            "--agencies",
            AGENCIES,
            "--allow-update",
            "--sequential",
        ],
        "Scraping notÃ­cias gov.br",
    )

    # Validar que notÃ­cias foram inseridas
    total_news = count_news()
    assert total_news > 0, "Nenhuma notÃ­cia foi scraped"
    print(f"\nğŸ“° Total de notÃ­cias scraped: {total_news}")

    # Validar datas
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute("""
        SELECT
            DATE(published_at) as date,
            COUNT(*) as count
        FROM news
        GROUP BY DATE(published_at)
        ORDER BY date
    """)
    results = cur.fetchall()
    cur.close()
    conn.close()

    print("\nğŸ“Š DistribuiÃ§Ã£o por data:")
    for date, count in results:
        print(f"   {date}: {count} notÃ­cias")

    print("\nâœ… FASE 2a COMPLETA: Scraping finalizado")


def test_02b_scrape_ebc(docker_services: None, env_vars: None) -> None:
    """
    FASE 2b: Scraping de noticias EBC (Agencia Brasil, TVBrasil).

    Nota: EBC scraping e mais lento que gov.br porque visita cada pagina de artigo.
    Para evitar timeout, usamos apenas 1 dia e 2 agencias.
    """
    print("\n" + "=" * 70)
    print("FASE 2b: Scraping EBC (agencia-brasil, tvbrasil)")
    print("=" * 70)

    # EBC scraping visita cada pagina de artigo, entao e mais lento.
    # Usamos EBC_DATE (data recente) para evitar timeout ao paginar.
    run_cli_command(
        [
            "scrape-ebc",
            "--start-date",
            EBC_DATE,
            "--end-date",
            EBC_DATE,
            "--agencies",
            EBC_AGENCIES,
            "--allow-update",
            "--sequential",
        ],
        f"Scraping noticias EBC (agencia-brasil e tvbrasil {EBC_DATE})",
    )

    # Validar que noticias EBC foram inseridas
    conn = get_db_connection()
    cur = conn.cursor()

    cur.execute("SELECT COUNT(*) FROM news WHERE agency_key IN ('agencia-brasil', 'tvbrasil')")
    ebc_count = cur.fetchone()[0]

    assert ebc_count > 0, "Nenhuma noticia EBC foi scraped"
    print(f"\nğŸ“° Total de noticias EBC scraped: {ebc_count}")

    cur.close()
    conn.close()

    print("\nâœ… FASE 2b COMPLETA: Scraping EBC finalizado")


def test_03_upload_cogfy(docker_services: None, env_vars: None) -> None:
    """
    FASE 3: Upload de notÃ­cias para Cogfy (AI enrichment).

    Requer COGFY_API_KEY configurado.
    """
    print("\n" + "=" * 70)
    print("FASE 3: Upload para Cogfy")
    print("=" * 70)

    # Verificar que COGFY_API_KEY estÃ¡ configurado
    if not os.getenv("COGFY_API_KEY"):
        pytest.fail(
            "COGFY_API_KEY nÃ£o configurado. Configure com: export COGFY_API_KEY=<sua-chave>"
        )

    run_cli_command(
        ["upload-cogfy", "--start-date", START_DATE, "--end-date", END_DATE],
        "Enviando notÃ­cias para Cogfy",
    )

    print("\nâœ… FASE 3 COMPLETA: Upload para Cogfy finalizado")
    print(f"\nâ¸ï¸  AGUARDANDO: Cogfy processarÃ¡ as notÃ­cias (~{COGFY_WAIT_TIME // 60} minutos)")
    print("   O Cogfy usa LLM para classificar temas e gerar resumos...")

    # Aguardar processamento do Cogfy
    print(f"\nâ³ Aguardando {COGFY_WAIT_TIME} segundos para processamento Cogfy...")
    for i in range(COGFY_WAIT_TIME // 60):
        time.sleep(60)
        remaining = COGFY_WAIT_TIME // 60 - i - 1
        print(f"   â±ï¸  {remaining} minutos restantes...")

    print("\nâœ… Tempo de espera completo. Prosseguindo para enrich...")


def test_04_enrich_themes(docker_services: None, env_vars: None) -> None:
    """
    FASE 4: Enrich com temas classificados pelo Cogfy.
    """
    print("\n" + "=" * 70)
    print("FASE 4: Enrich themes (baixar de Cogfy)")
    print("=" * 70)

    # Verificar que COGFY_API_KEY estÃ¡ configurado
    if not os.getenv("COGFY_API_KEY"):
        pytest.fail("COGFY_API_KEY nÃ£o configurado")

    run_cli_command(
        ["enrich", "--start-date", START_DATE, "--end-date", END_DATE],
        "Baixando temas enriquecidos do Cogfy",
    )

    # Validar enriquecimento
    conn = get_db_connection()
    cur = conn.cursor()

    # Contar notÃ­cias com tema
    cur.execute("SELECT COUNT(*) FROM news WHERE most_specific_theme_id IS NOT NULL")
    enriched_count = cur.fetchone()[0]

    # Contar notÃ­cias com summary
    cur.execute("SELECT COUNT(*) FROM news WHERE summary IS NOT NULL AND summary != ''")
    summary_count = cur.fetchone()[0]

    total = count_news(conn)

    cur.close()
    conn.close()

    print("\nğŸ“Š Resultados do enriquecimento:")
    print(f"   Total de notÃ­cias: {total}")
    print(f"   Com tema classificado: {enriched_count} ({enriched_count / total * 100:.1f}%)")
    print(f"   Com AI summary: {summary_count} ({summary_count / total * 100:.1f}%)")

    assert enriched_count > 0, "Nenhuma notÃ­cia foi enriquecida com temas"

    print("\nâœ… FASE 4 COMPLETA: Enriquecimento finalizado")


def test_05_generate_embeddings(docker_services: None, env_vars: None) -> None:
    """
    FASE 5: GeraÃ§Ã£o de embeddings semÃ¢nticos via API Cloud Run.

    Requer API de embeddings rodando (Cloud Run).
    """
    print("\n" + "=" * 70)
    print("FASE 5: GeraÃ§Ã£o de embeddings")
    print("=" * 70)

    run_cli_command(
        [
            "generate-embeddings",
            "--start-date",
            START_DATE,
            "--end-date",
            END_DATE,
            "--batch-size",
            "32",
        ],
        "Gerando embeddings via API Cloud Run",
    )

    # Validar embeddings
    conn = get_db_connection()
    cur = conn.cursor()

    # Contar notÃ­cias com embedding
    cur.execute("SELECT COUNT(*) FROM news WHERE content_embedding IS NOT NULL")
    embeddings_count = cur.fetchone()[0]

    total = count_news(conn)

    cur.close()
    conn.close()

    print("\nğŸ“Š Resultados dos embeddings:")
    print(f"   Total de notÃ­cias: {total}")
    print(f"   Com embeddings: {embeddings_count} ({embeddings_count / total * 100:.1f}%)")

    assert embeddings_count > 0, "Nenhum embedding foi gerado"

    print("\nâœ… FASE 5 COMPLETA: Embeddings gerados")


def test_06_sync_typesense_full(docker_services: None, env_vars: None) -> None:
    """
    FASE 6: Sync full para Typesense (com embeddings).

    - Deleta collection 'news' existente
    - Recria collection com schema atualizado
    - Popula com todas as notÃ­cias do PostgreSQL
    """
    print("\n" + "=" * 70)
    print("FASE 6: Sync full Typesense")
    print("=" * 70)

    # Deletar collection existente
    print("\nğŸ—‘ï¸  Deletando collection 'news' (se existir)...")
    try:
        subprocess.run(
            [
                "poetry",
                "run",
                "data-platform",
                "typesense-delete",
                "--collection-name",
                "news",
                "--confirm",
            ],
            capture_output=True,
            timeout=30,
        )
        print("   âœ… Collection deletada")
    except subprocess.SubprocessError:
        print("   â„¹ï¸  Collection nÃ£o existia ou jÃ¡ foi deletada")

    # Sync full (embeddings are always included now)
    run_cli_command(
        [
            "sync-typesense",
            "--start-date",
            START_DATE,
            "--end-date",
            END_DATE,
            "--full-sync",
        ],
        "Executando sync full (recria collection e popula)",
    )

    # Validar dados no Typesense
    print("\nğŸ“Š Validando dados no Typesense...")

    # Testar busca
    response = requests.get(
        f"http://{TYPESENSE_HOST}:{TYPESENSE_PORT}/collections/news/documents/search",
        params={"q": "educaÃ§Ã£o", "query_by": "title"},
        headers={"X-TYPESENSE-API-KEY": TYPESENSE_API_KEY},
        timeout=10,
    )

    assert response.status_code == 200, f"Erro ao buscar no Typesense: {response.status_code}"

    data = response.json()
    found = data.get("found", 0)
    print(f"   ğŸ” Busca por 'educaÃ§Ã£o': {found} resultados encontrados")

    # Listar collections
    print("\nğŸ“‹ Listando collections...")
    result = subprocess.run(
        ["poetry", "run", "data-platform", "typesense-list"],
        capture_output=True,
        text=True,
        timeout=30,
    )
    print(f"   {result.stdout}")

    print("\nâœ… FASE 6 COMPLETA: Typesense populado")


def test_07_validate_final_state(docker_services: None, env_vars: None) -> None:
    """
    FASE 7: ValidaÃ§Ã£o final do estado completo do sistema.

    Verifica:
    - PostgreSQL tem notÃ­cias com todos os enriquecimentos
    - Typesense estÃ¡ acessÃ­vel e tem dados
    - Embeddings foram gerados
    - Temas foram enriquecidos
    """
    print("\n" + "=" * 70)
    print("FASE 7: ValidaÃ§Ã£o final")
    print("=" * 70)

    conn = get_db_connection()
    cur = conn.cursor()

    # EstatÃ­sticas gerais
    cur.execute("SELECT COUNT(*) FROM news")
    total_news = cur.fetchone()[0]

    cur.execute("SELECT COUNT(*) FROM news WHERE most_specific_theme_id IS NOT NULL")
    with_themes = cur.fetchone()[0]

    cur.execute("SELECT COUNT(*) FROM news WHERE summary IS NOT NULL AND summary != ''")
    with_summary = cur.fetchone()[0]

    cur.execute("SELECT COUNT(*) FROM news WHERE content_embedding IS NOT NULL")
    with_embeddings = cur.fetchone()[0]

    cur.execute("SELECT COUNT(*) FROM agencies")
    total_agencies = cur.fetchone()[0]

    cur.execute("SELECT COUNT(*) FROM themes")
    total_themes = cur.fetchone()[0]

    # EstatÃ­sticas por agency
    cur.execute("""
        SELECT agency_key, COUNT(*) as count
        FROM news
        GROUP BY agency_key
        ORDER BY count DESC
    """)
    by_agency = cur.fetchall()

    cur.close()
    conn.close()

    # Imprimir relatÃ³rio
    print("\n" + "=" * 70)
    print("ğŸ“Š RELATÃ“RIO FINAL")
    print("=" * 70)
    print("\nğŸ—„ï¸  POSTGRESQL:")
    print(f"   Total de notÃ­cias: {total_news}")
    print(f"   Com temas: {with_themes} ({with_themes / total_news * 100:.1f}%)")
    print(f"   Com AI summary: {with_summary} ({with_summary / total_news * 100:.1f}%)")
    print(f"   Com embeddings: {with_embeddings} ({with_embeddings / total_news * 100:.1f}%)")
    print(f"   Total de agencies: {total_agencies}")
    print(f"   Total de themes: {total_themes}")

    print("\nğŸ“ˆ DISTRIBUIÃ‡ÃƒO POR AGENCY:")
    for agency, count in by_agency:
        print(f"   {agency}: {count} notÃ­cias")

    ebc_total = sum(c for a, c in by_agency if a in ("agencia-brasil", "tvbrasil"))
    if ebc_total > 0:
        print(f"\nğŸ“º EBC (total: {ebc_total}):")
        for agency, count in by_agency:
            if agency in ("agencia-brasil", "tvbrasil"):
                print(f"   {agency}: {count} notÃ­cias")

    print("\nğŸ” TYPESENSE:")
    # Testar acesso ao Typesense
    try:
        response = requests.get(
            f"http://{TYPESENSE_HOST}:{TYPESENSE_PORT}/collections/news/documents/search",
            params={"q": "*", "per_page": "1"},
            headers={"X-TYPESENSE-API-KEY": TYPESENSE_API_KEY},
            timeout=5,
        )
        data = response.json()
        ts_found = data.get("found", 0)
        print(f"   Total de documentos: {ts_found}")
        print("   Status: âœ… AcessÃ­vel")
    except Exception as e:
        print(f"   Status: âŒ Erro: {e}")

    print("\nâ±ï¸  PERÃODO TESTADO:")
    print(f"   InÃ­cio: {START_DATE}")
    print(f"   Fim: {END_DATE}")
    print(f"   Gov.br agencies: {AGENCIES}")
    print(f"   EBC agencies: {EBC_AGENCIES}")

    print("\n" + "=" * 70)
    print("âœ… BATERIA DE TESTES COMPLETA!")
    print("=" * 70)

    # AsserÃ§Ãµes finais
    assert total_news > 0, "Sistema vazio - nenhuma notÃ­cia"
    assert with_themes > 0, "Nenhuma notÃ­cia foi enriquecida com temas"
    assert with_embeddings > 0, "Nenhum embedding foi gerado"
    assert total_agencies > 0, "Nenhuma agency foi carregada"
    assert total_themes > 0, "Nenhum theme foi carregado"


# ==============================================================================
# EXECUÃ‡ÃƒO STANDALONE
# ==============================================================================
if __name__ == "__main__":
    print("""
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    BATERIA DE TESTES - DATA PLATFORM
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    Este script testa o pipeline completo:
    1. Populate master data (agencies, themes)
    2. Scraping (MEC, GESTAO, CGU)
    3. Upload para Cogfy
    4. Enrich themes
    5. Generate embeddings
    6. Sync Typesense
    7. ValidaÃ§Ã£o final

    Requisitos:
    - Docker containers rodando
    - COGFY_API_KEY configurado
    - Cloud Run API de embeddings acessÃ­vel

    Execute com:
        poetry run pytest tests/integration/test_full_pipeline.py -v -s

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
